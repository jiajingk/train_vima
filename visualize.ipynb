{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_paths = list(glob(os.path.join('.', 'train_*.csv')))\n",
    "valid_csv_paths = list(glob(os.path.join('.', 'valid_*.csv')))\n",
    "assert len(train_csv_paths) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_csv_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = {col: col.replace('validation__', '') for col in df.columns if col.startswith('validation__')}\n",
    "df = df.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unweighted_sample_cols = [\n",
    "    str(col) for col in df.columns if 'unweigted_sample_loss' in col\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = sorted(list(set(df.epoch_id)))\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unweighted_losses = []\n",
    "for i in epochs:\n",
    "    loss = df.loc[df.epoch_id == i].iloc[:, df.columns.isin(unweighted_sample_cols)].sum(axis=1).mean()\n",
    "    unweighted_losses.append(loss)\n",
    "plt.plot(unweighted_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns)\n",
    "rotation_cols = list(col for col in cols if '_rotation_' in col and 'sample_loss' in col)\n",
    "position_cols = list(col for col in cols if '_position_' in col and 'sample_loss' in col)\n",
    "fig = plt.figure()\n",
    "mean_loss = df[rotation_cols + position_cols + ['epoch_id']].groupby('epoch_id').mean()\n",
    "ax = mean_loss.plot(ylabel='cross_entropy_loss')\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_task_loss = df[rotation_cols + position_cols + ['task']].groupby('task').sum().sum(axis=1).sort_values()\n",
    "per_task_loss / per_task_loss.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from playground.typing import TrainParam, DatasetParam\n",
    "\n",
    "def get_lr_param():\n",
    "    return {\n",
    "        \"warmup_end_at_iters\": 7000,\n",
    "        \"flatten_end_at_iters\": 240000,\n",
    "        \"lr_decay_end_at_iters\": 960000,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"min_lr\": 1e-7, \n",
    "    }\n",
    "\n",
    "\n",
    "def get_optimizer_param():\n",
    "    return {\n",
    "        \"clip_norm\": 1.0,\n",
    "        \"inital_lr\": get_lr_param()[\"learning_rate\"],\n",
    "        \"optimizer_name\": \"AdamW\",\n",
    "        \"weight_decay\": 0.0\n",
    "    }\n",
    "\n",
    "def get_dataset_param():\n",
    "    return  {\n",
    "        \"data_pct_usage\": 1.0,\n",
    "        \"total_data_size_per_task\": 40000,\n",
    "        \"validation_pct\": 0.00,\n",
    "        \"source\": \"s3://vima\",\n",
    "        \"tasks\": [\n",
    "            \"follow_order\",\n",
    "            \"manipulate_old_neighbor\",\n",
    "            \"novel_adj\",\n",
    "            \"novel_noun\",\n",
    "            \"pick_in_order_then_restore\",\n",
    "            \"rearrange_then_restore\",\n",
    "            \"rearrange\",\n",
    "            \"rotate\",\n",
    "            \"same_profile\",\n",
    "            \"scene_understanding\",\n",
    "            \"simple_manipulation\",\n",
    "            \"sweep_without_exceeding\",\n",
    "            \"twist\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def get_train_param():\n",
    "    return {\n",
    "        \"model_size\": \"2M\",\n",
    "        \"total_epoch\": 50,\n",
    "        \"local_batch_size\": 128,\n",
    "        \"distributed\": False,\n",
    "    }\n",
    "def get_lr(it: int) -> float:\n",
    "    lr_param = get_lr_param()\n",
    "    warmup_iters = lr_param[\"warmup_end_at_iters\"]\n",
    "    flatten_iters = lr_param[\"flatten_end_at_iters\"]\n",
    "    learning_rate = lr_param[\"learning_rate\"]\n",
    "    lr_decay_iters = lr_param[\"lr_decay_end_at_iters\"]\n",
    "    min_lr = lr_param[\"min_lr\"]\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    if warmup_iters <= it < flatten_iters:\n",
    "        return learning_rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - flatten_iters) / (lr_decay_iters - flatten_iters)\n",
    "    assert 0 <= decay_ratio <= 1, f\"{decay_ratio = }, {it = }\"\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "\n",
    "def get_batch_per_epoch(\n",
    "        dataset_param: DatasetParam,\n",
    "        train_param: TrainParam,\n",
    "        is_train: bool = True\n",
    "    ):\n",
    "    if is_train:\n",
    "        scaling = 1.0\n",
    "    else:\n",
    "        scaling = dataset_param[\"validation_pct\"]\n",
    "    epoch_size = ( \n",
    "        int(\n",
    "            dataset_param[\"total_data_size_per_task\"] \n",
    "            * scaling \n",
    "            * len(dataset_param[\"tasks\"]\n",
    "        ) \n",
    "        * dataset_param[\"data_pct_usage\"]) \n",
    "    )\n",
    "    batch_size = (\n",
    "        train_param[\"local_batch_size\"] \n",
    "            if train_param[\"distributed\"] is False \n",
    "            else train_param[\"local_batch_size\"] * 1\n",
    "    )\n",
    "    if epoch_size % batch_size != 0:\n",
    "        return epoch_size // batch_size + 1\n",
    "    return epoch_size // batch_size\n",
    "\n",
    "def get_total_batch_count(\n",
    "        dataset_param: DatasetParam,\n",
    "        train_param: TrainParam,\n",
    "        batch_id: int, \n",
    "        epoch_id: int,\n",
    "        is_train: bool = True\n",
    "    ) -> int:\n",
    "    batch_count_per_epoch = get_batch_per_epoch(dataset_param, train_param, is_train)\n",
    "    current_total_batch_count = batch_id + epoch_id * batch_count_per_epoch\n",
    "    return current_total_batch_count\n",
    "\n",
    "def measure_lr(\n",
    "        dataset_param: DatasetParam,\n",
    "        train_param: TrainParam,\n",
    "        batch_id: int, \n",
    "        epoch_id: int\n",
    "    ):\n",
    "    current_total_batch_count = get_total_batch_count(\n",
    "        dataset_param, \n",
    "        train_param, \n",
    "        batch_id, \n",
    "        epoch_id, \n",
    "        is_train=True\n",
    "    )\n",
    "    return get_lr(current_total_batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lrs():\n",
    "    lrs = []\n",
    "    for epoch_id in range(get_train_param()[\"total_epoch\"]):\n",
    "        for batch_id in range(\n",
    "            get_batch_per_epoch(\n",
    "                get_dataset_param(),\n",
    "                get_train_param(),\n",
    "            )\n",
    "        ):\n",
    "            lrs.append(\n",
    "                measure_lr(\n",
    "                    get_dataset_param(),\n",
    "                    get_train_param(),\n",
    "                    batch_id,\n",
    "                    epoch_id\n",
    "                )\n",
    "            )\n",
    "    return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = get_lrs()\n",
    "print(len(lrs))\n",
    "_ = plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('eval_2024-05-19_zesty-microwave-656_14_2024-05-23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ids = {\n",
    "    \"visual_manipulation\": \"01\",\n",
    "    \"scene_understanding\": \"02\",\n",
    "    \"rotate\": \"03\",\n",
    "    \"rearrange\": \"04\",\n",
    "    \"rearrange_then_restore\": \"05\",\n",
    "    \"novel_adj\": \"06\",\n",
    "    \"novel_noun\": \"07\",\n",
    "    \"novel_adj_and_noun\": \"08\",\n",
    "    \"twist\": \"09\",\n",
    "    \"follow_motion\": \"10\",\n",
    "    \"follow_order\": \"11\",\n",
    "    \"sweep_without_exceeding\": \"12\",\n",
    "    \"sweep_without_touching\": \"13\",\n",
    "    \"same_texture\": \"14\",\n",
    "    \"same_shape\": \"15\",\n",
    "    \"manipulate_old_neighbor\": \"16\",\n",
    "    \"pick_in_order_then_restore\": \"17\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eval_success_rates = {\n",
    "    task_ids[group.iloc[0]['task']]: round(float(group[['sucess']].mean().iloc[0] * 100), 1)\n",
    "        for _, group in df.groupby('task')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('01', 56.0),\n",
       " ('02', 50.0),\n",
       " ('03', 29.0),\n",
       " ('04', 35.0),\n",
       " ('05', 18.0),\n",
       " ('06', 42.0),\n",
       " ('07', 60.0),\n",
       " ('09', 1.0),\n",
       " ('11', 77.0),\n",
       " ('12', 65.0),\n",
       " ('15', 49.0),\n",
       " ('16', 35.0),\n",
       " ('17', 5.0)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(eval_success_rates.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.15384615384615"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(eval_success_rates.values()) / len(eval_success_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('01', 56.0),\n",
       " ('02', 50.0),\n",
       " ('03', 29.0),\n",
       " ('04', 35.0),\n",
       " ('05', 18.0),\n",
       " ('06', 42.0),\n",
       " ('07', 60.0),\n",
       " ('09', 1.0),\n",
       " ('11', 77.0),\n",
       " ('12', 65.0),\n",
       " ('15', 49.0),\n",
       " ('16', 35.0),\n",
       " ('17', 5.0)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(eval_success_rates.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "mode = 'train'\n",
    "file_pattern = f'logs\\\\{mode}_*.csv'\n",
    "csv_files = glob.glob(file_pattern)\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "solution_df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cols = [col for col in df.columns if 'unweigted_sample_loss' in col]\n",
    "position_loss_cols = [col for col in loss_cols if 'position' in col]\n",
    "rotation_loss_cols = [col for col in loss_cols if 'rotation' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename = {\n",
    "    old_col_name: old_col_name.replace(\"unweigted_sample_loss__\", \"\") \n",
    "        for old_col_name in loss_cols\n",
    "}\n",
    "df[loss_cols].rename(columns=rename).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_name, task_df in df[position_loss_cols + ['task']].groupby('task'):\n",
    "    rename = {\n",
    "        old_col_name: old_col_name.replace(\"unweigted_sample_loss__\", \"\") \n",
    "            for old_col_name in position_loss_cols\n",
    "    }\n",
    "    print(task_name)\n",
    "    print(task_df[position_loss_cols].rename(columns=rename).describe())\n",
    "    print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sum_loss'] = df[position_loss_cols].sum(axis=1)\n",
    "df.groupby('task')['sum_loss'].describe().T.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from typing import List, Literal, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mode = 'train'\n",
    "i = 31\n",
    "    \n",
    "file_pattern = f'logs/{mode}_{i}_*.csv'\n",
    "csv_files = glob.glob(file_pattern)\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cols = [col for col in df.columns if 'unweigted_sample_loss' in col]\n",
    "position_loss_cols = [col for col in loss_cols if 'position' in col]\n",
    "rotation_loss_cols = [col for col in loss_cols if 'rotation' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename = {\n",
    "    old_col_name: old_col_name.replace(\"unweigted_sample_loss__\", \"\") \n",
    "        for old_col_name in loss_cols\n",
    "}\n",
    "df[loss_cols].rename(columns=rename).describe().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = set(\n",
    "    [\n",
    "        str(os.path.basename(file_path))\n",
    "        .split('.')[0]\n",
    "        .split('_')[2] \n",
    "            for file_path in glob.glob('logs/*.csv')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace_paths(\n",
    "        exp_id: str, \n",
    "        mode: str\n",
    "    ):\n",
    "    return [\n",
    "        exp\n",
    "            for exp in sorted(\n",
    "                glob.glob(f'logs/{mode}_*_{exp_id}.csv'),\n",
    "                key = lambda x: int(x.split('_')[1])\n",
    "            )\n",
    "    ]\n",
    "\n",
    "def trace_epoch(\n",
    "        exp_id: str, \n",
    "        mode: str = 'train'\n",
    "    ) -> List[pd.DataFrame]:\n",
    "    return [\n",
    "        pd.read_csv(exp)\n",
    "            for exp in \n",
    "                get_trace_paths(exp_id, mode)\n",
    "    ]\n",
    "\n",
    "def trace_ddp(exp_ids: List[str], epoch: int, mode: str = 'train') -> pd.DataFrame:\n",
    "    exp_paths = [\n",
    "        os.path.join('logs', f'{mode}_{epoch}_{exp_id}.csv') for exp_id in exp_ids\n",
    "    ]\n",
    "    exp_paths = filter(lambda x: os.path.exists(x), exp_paths)\n",
    "    dataframes = [\n",
    "        pd.read_csv(exp_path) for exp_path in exp_paths\n",
    "    ]\n",
    "    df = pd.concat(dataframes, ignore_index=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    trace_ddp(exps, i, 'train') for i in range(30)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aggregator = Literal['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "Transform = Callable[[pd.DataFrame], pd.DataFrame]\n",
    "def measure(\n",
    "        df: pd.DataFrame, \n",
    "        transform: Transform, \n",
    "        attr: str, \n",
    "        aggr: Aggregator\n",
    "    ) -> float:\n",
    "    df = transform(df)\n",
    "    describe = df.describe()\n",
    "    if attr not in describe.columns:\n",
    "        describe = describe.T\n",
    "    return describe[attr][aggr]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\n",
    "    'pose0_position_0', 'pose0_position_1', 'pose1_position_0',\n",
    "    'pose1_position_1', 'pose0_rotation_0', 'pose0_rotation_1',\n",
    "    'pose0_rotation_2', 'pose0_rotation_3', 'pose1_rotation_0',\n",
    "    'pose1_rotation_1', 'pose1_rotation_2', 'pose1_rotation_3'\n",
    "]\n",
    "tasks = [\n",
    "    'follow_order', 'manipulate_old_neighbor', 'novel_adj', 'novel_noun',\n",
    "    'pick_in_order_then_restore', 'rearrange', 'rearrange_then_restore',\n",
    "    'rotate', 'same_profile', 'scene_understanding', 'simple_manipulation',\n",
    "    'sweep_without_exceeding', 'twist'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_attr_transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    loss_cols = [col for col in df.columns if 'unweigted_sample_loss' in col]\n",
    "    rename = {\n",
    "        old_col_name: old_col_name.replace(\"unweigted_sample_loss__\", \"\") \n",
    "            for old_col_name in loss_cols\n",
    "    }\n",
    "    return df[loss_cols].rename(columns=rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure(solution_df, per_attr_transform, 'pose0_position_0', 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in attributes:\n",
    "    print(attr, measure(df, per_attr_transform, attr, 'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_task_transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    loss_cols = [col for col in df.columns if 'unweigted_sample_loss' in col]\n",
    "    df['sum_loss'] = df[loss_cols].sum(axis=1)\n",
    "    return df.groupby('task')['sum_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasks:\n",
    "    print(task, measure(\n",
    "        df, \n",
    "        per_task_transform, \n",
    "        task, \n",
    "        'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for task in attributes:\n",
    "    print(task, measure(\n",
    "        solution_df.loc[\n",
    "            (solution_df['unweigted_sample_loss__pose0_position_0'] < 3.9) &\n",
    "            (solution_df['unweigted_sample_loss__pose0_position_1'] < 4.6) &\n",
    "            (solution_df['unweigted_sample_loss__pose1_position_0'] < 3.9) &\n",
    "            (solution_df['unweigted_sample_loss__pose1_position_1'] < 4.6) &\n",
    "            (solution_df['unweigted_sample_loss__pose0_rotation_0'] < 3.9) &\n",
    "            (solution_df['unweigted_sample_loss__pose0_rotation_1'] < 3.9) &\n",
    "            (solution_df['unweigted_sample_loss__pose0_rotation_2'] < 3.9) &\n",
    "            (solution_df['unweigted_sample_loss__pose0_rotation_3'] < 3.9) &\n",
    "            (solution_df['unweigted_sample_loss__pose1_rotation_0'] < 3.9) &\n",
    "            (solution_df['unweigted_sample_loss__pose1_rotation_1'] < 3.9) &\n",
    "            (solution_df['unweigted_sample_loss__pose1_rotation_2'] < 3.9) &\n",
    "            (solution_df['unweigted_sample_loss__pose1_rotation_3'] < 3.9) \n",
    "        ], \n",
    "        per_attr_transform, \n",
    "        task, \n",
    "        'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure(solution_df, per_task_transform, 'follow_order', 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_order = [measure(df, per_task_transform, 'follow_order', 'mean') for df in dfs]\n",
    "follow_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace Accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('eval_2024-05-19_zesty-microwave-656_14_2024-05-23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def per_trace_accu(row: pd.Series, col: str, threshold: float) -> float:\n",
    "    correct_trace: List[int] = []\n",
    "    for t_step in range(10):\n",
    "        ground_truth = row.loc[f'action_trace__{t_step}__oracle_action__{col}']\n",
    "        prediction = row.loc[f'action_trace__{t_step}__policy_action__{col}']\n",
    "        if pd.isna(ground_truth) and pd.isna(prediction):\n",
    "            break\n",
    "        if pd.isna(ground_truth) or pd.isna(prediction):\n",
    "            correct_trace.append(0)\n",
    "            continue\n",
    "        \n",
    "        correct_trace.append(int(abs(ground_truth - prediction) <= threshold))\n",
    "    if len(correct_trace) < 0:\n",
    "        return 0\n",
    "    return sum(correct_trace) / len(correct_trace)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\n",
    "    'pose0_position_0', 'pose0_position_1', 'pose1_position_0',\n",
    "    'pose1_position_1', 'pose0_rotation_0', 'pose0_rotation_1',\n",
    "    'pose0_rotation_2', 'pose0_rotation_3', 'pose1_rotation_0',\n",
    "    'pose1_rotation_1', 'pose1_rotation_2', 'pose1_rotation_3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "accus = {\n",
    "    attribute: df.apply(partial(per_trace_accu, col=attribute, threshold=0), axis=1).mean()\n",
    "    for attribute in attributes \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3358479853479854"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_accus = [accus[key] for key in accus if 'position' in key]\n",
    "rotation_accus = [accus[key] for key in accus if 'position' in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose0_position_0 0.9\n",
      "pose0_position_1 0.81\n",
      "pose1_position_0 0.95\n",
      "pose1_position_1 0.83\n",
      "pose0_rotation_0 1.0\n",
      "pose0_rotation_1 1.0\n",
      "pose0_rotation_2 1.0\n",
      "pose0_rotation_3 1.0\n",
      "pose1_rotation_0 1.0\n",
      "pose1_rotation_1 1.0\n",
      "pose1_rotation_2 0.09\n",
      "pose1_rotation_3 0.12\n"
     ]
    }
   ],
   "source": [
    "for attribute in attributes:\n",
    "    print(attribute, df.apply(partial(per_trace_accu, col=attribute, threshold=2), axis=1).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vima_reproduce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
